import doctest
import os
import logging
from collections import Counter
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix
from config import DATA_FOLDER_PATH, N_FOLDS, TEST_SIZE, RANDOM_SEED

class IdentityTransformer:
    """placeholder of transformer, do not apply any transformation
    >>> X = np.array([0., 1.])
    >>> it = IdentityTransformer().fit(X)
    >>> Xp = it.transform(X)
    >>> np.array_equal(Xp, X)
    True
    """
    def __init__(self, name=''):
        self.name = name

    def fit(self, X, y=None):
        self.features = [self.name]
        return self

    def transform(self, X, y=None):
        return X

    def fit_transform(self, X, y=None):
        self.fit(X, y)
        return self.transform(X, y)    

class SingleMinMaxScaler:
    """wrapper of sklearn MinMaxScaler so we can input single column X
    >>> X = np.array([0., 1.])
    >>> mms = SingleMinMaxScaler(feature_range=(-1, 1)).fit(X)
    >>> Xp = mms.transform(X)
    >>> np.array_equal(Xp, np.array([-1., 1.]))
    True
    """
    def __init__(self, name='', **kwargs):
        self.scaler = MinMaxScaler(**kwargs)
        self.name = 'min_max_scaled_' + name

    def fit(self, X, y=None):
        self.scaler.fit(X.astype(np.float64).reshape(-1, 1), y)
        self.features = [self.name]
        return self

    def transform(self, X, y=None):
        return self.scaler.transform(X.astype(np.float64).reshape(-1, 1))[:, 0]

    def fit_transform(self, X, y=None):
        self.fit(X, y)
        return self.transform(X, y)

class SingleOneHotEncoder:
    """wrapper of sklearn OneHotEncoder so we can input single column X
    >>> X = np.array([0, 1, 0])
    >>> ohe = SingleOneHotEncoder('feature').fit(X)
    >>> Xp = ohe.transform(X)
    >>> np.array_equal(Xp, np.array([[1., 0.], [0., 1.], [1., 0.]]))
    True
    >>> ohe.features
    ['one_hot_encoding_feature_0', 'one_hot_encoding_feature_1']
    """
    def __init__(self, name='', **kwargs):
        self.encoder = OneHotEncoder(**kwargs)
        self.name = 'one_hot_encoding_' + name

    def fit(self, X, y=None):
        self.encoder.fit(X.reshape(-1, 1), y)
        self.features = [self.name + '_' + str(i) for i in range(self.encoder.n_values_[0])]
        return self

    def transform(self, X, y=None):
        return self.encoder.transform(X.reshape(-1, 1)).toarray()

    def fit_transform(self, X, y=None):
        self.fit(X, y)
        return self.transform(X, y)

class NumericalEncoder:
    """
    encoder categorical features into numerical features
    frequency below threshold will be consider as outlier (label -1)

    >>> X = np.array([1, 1, 1, 1, 2, 2, 2, 4, 2, 2, 99])
    >>> y = np.array([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]) # samples labels (whether is malware or not!)
    >>> ne = NumericalEncoder(3).fit(X, y)
    >>> np.array_equal(ne.transform(X, y), np.array([0.25, 0.25, 0.25, 0.25, 0.8, 0.8, 0.8, 0.5, 0.8, 0.8, 0.5]))
    True

    For categories not in the training set, it give output of the outlier category
    >>> np.array_equal(ne.transform(np.array([1988, 2017])), np.array([0.5, 0.5]))
    True

    For existing category, like 1, we expect the output to be 0.25
    >>> ne.transform(np.array([1]))[0]
    0.25
    """
    def __init__(self, threshold, name=''):
        self.threshold = threshold
        self.valid_cats = set()
        self.name = 'numerical_encoder_' + name

    def fit(self, X, y):
        """
        X: numpy.array, (N,), categorical feature values
        y: numpy.array, (N,), binary labels
        """
        if len(X.shape) > 1:
            raise ValueError('X should be 1d array, but got array shape %s' % str(X.shape))

        cnt_dict = Counter(X)
        self.valid_cats = set([k for k, v in Counter(X).items() if v >= self.threshold])

        df = pd.DataFrame({'cat': X, 'label': y})
        df['clean_cat'] = df['cat'].apply(lambda x: x if x in self.valid_cats else -1)

        aux_df = 1. * df.groupby('clean_cat')['label'].sum() / df.groupby('clean_cat').size()

        self.aux_df = aux_df.to_frame('label_ratio').reset_index()

        self.features = [self.name]

        return self

    def transform(self, X, y=None):
        """
        X: numpy.array, (N,), categorical feature values
        """
        if len(X.shape) > 1:
            raise ValueError('X should be 1d array, but got array shape %s' % str(X.shape))

        df = pd.DataFrame({'cat': X})
        df['clean_cat'] = df['cat'].apply(lambda x: x if x in self.valid_cats else -1)
        
        df = pd.merge(df, self.aux_df, on='clean_cat', how='left')
        if self.threshold <= 1:
            df['label_ratio'].fillna(0, inplace=True)
        return df['label_ratio'].values

    def fit_transform(self, X, y):
        self.fit(X, y)
        return self.transform(X)


class IconData:
    """read data
    >>> data = IconData(DATA_FOLDER_PATH)
    >>> len(data.X) == 2271
    True

    use partition_data method to split the data to train, validation and test
    >>> data.partition_data(test_size=0.2, n_splits=4, random_state=0)
    >>> data.X_test.shape == (455, 10)
    True
    >>> data.y_test.shape == (455,)
    True

    you can get hash, y, X of train and valid for each shard by iterating the object
    >>> len(list(data))
    4
    >>> hash_tr_i, y_tr_i, X_tr_i, hash_va_i, y_va_i, X_va_i = data[2]
    >>> X_tr_i.shape == (1363, 10)
    True
    >>> X_va_i.shape == (453, 10)
    True

    check whether any sample missing in the partition
    >>> len(data.y) == len(data.y_test) + len(y_tr_i) + len(y_va_i)
    True
    """
    def __init__(self, data_folder_path=DATA_FOLDER_PATH):
        self.path = data_folder_path
        self.read_data()

    def read_data(self):
        features = []
        df_iconCluster = pd.read_csv(os.path.join(self.path, 'icon_clusters.csv'))
        df_iconCluster.columns = ['indx', 'hash', 'clusterID', 'outlierInd']
        df_iconCluster.drop('indx', axis=1, inplace=True)
        features.append('clusterID')

        section_feature_cols = ['data_VS', 'data_SR', 'data_entropy', 'rsrc_VS', 'rsrc_SR', 'rsrc_entropy', 'text_VS', 'text_SR', 'text_entropy', 'hash']
        df_section_feature_bad = pd.read_csv(os.path.join(self.path, 'section_feature_bad.csv'))
        df_section_feature_good = pd.read_csv(os.path.join(self.path, 'section_feature_good.csv'))
        df_section_feature_bad.columns = section_feature_cols
        df_section_feature_good.columns = section_feature_cols
        df_section_feature_bad['label'] = 1
        df_section_feature_good['label'] = 0
        df_section_feature = pd.concat((df_section_feature_bad, df_section_feature_good), ignore_index=True)
        features.extend(['data_VS', 'data_SR', 'data_entropy', 'rsrc_VS', 'rsrc_SR', 'rsrc_entropy', 'text_VS', 'text_SR', 'text_entropy'])

        df_data = pd.merge(df_section_feature, df_iconCluster, on='hash')
        self.hash = df_data['hash'].values
        self.y = df_data['label'].values
        self.X = df_data[features].values
        self.features = features

    def partition_data(self, test_size=0.2, n_splits=4, random_state=0):
        idx = np.arange(len(self.y))
        idx_train, idx_test = train_test_split(idx,
                                test_size=test_size,
                                random_state=random_state,
                                stratify=self.y)
        
        self.hash_train = self.hash[idx_train]
        self.y_train = self.y[idx_train]
        self.X_train = self.X[idx_train]

        self.hash_test = self.hash[idx_test]
        self.y_test = self.y[idx_test]
        self.X_test = self.X[idx_test]

        skf = StratifiedKFold(n_splits=n_splits)
        self.hash_tr = []
        self.y_tr = []
        self.X_tr = []
        self.hash_va = []
        self.y_va = []
        self.X_va = []
        for train_idx, valid_idx in skf.split(self.X_train, self.y_train):
            self.hash_tr.append(self.hash_train[train_idx])
            self.y_tr.append(self.y_train[train_idx])
            self.X_tr.append(self.X_train[train_idx])
            self.hash_va.append(self.hash_train[valid_idx])
            self.y_va.append(self.y_train[valid_idx])
            self.X_va.append(self.X_train[valid_idx])

    def __getitem__(self, i):
        return self.hash_tr[i], self.y_tr[i], self.X_tr[i], self.hash_va[i], self.y_va[i], self.X_va[i]


class FeatureEngineeringPipeline:
    """simple feature engineering pipeline. features not in guides will not be processed and output.
    >>> input_features = ['a', 'b']
    >>> guides = {'a': [{'transformer': NumericalEncoder, 'params': {'threshold': 1}}, {'transformer': IdentityTransformer, 'params': {}}], 'b': {'transformer': SingleMinMaxScaler, 'params': {'feature_range': (-1, 1)}}}
    >>> X = np.array([[1, 2], [1, -2], [2, -1], [2, 1]])
    >>> y = np.array([0, 1, 0, 1])
    >>> fep = FeatureEngineeringPipeline(guides, input_features).fit(X, y)
    >>> Xp = fep.transform(X, y)
    >>> np.array_equal(Xp, np.array([[0.5, 0.5, 0.5, 0.5], [1., 1., 2., 2.], [1, -1, -0.5, 0.5]]).T)
    True
    >>> fep.features
    ['numerical_encoder_a', 'a', 'min_max_scaled_b']
    """
    def __init__(self, guides, input_features):
        self.transformers = {}
        for k, v in guides.items():
            self.transformers[k] = []
            v_iter = v if isinstance(v, list) else [v]
            for v_elem in v_iter:
                self.transformers[k].append(v_elem['transformer'](name=k, **v_elem['params']))
        self.input_features = input_features
        self.features = []

    def fit(self, X, y):
        for i, feature in enumerate(self.input_features):
            if feature in self.transformers:
                for trans in self.transformers[feature]:
                    trans.fit(X[:, i], y)
                    self.features.extend(trans.features)
        return self

    def transform(self, X, y=None):
        Xp = []
        for i, feature in enumerate(self.input_features):
            if feature in self.transformers:
                for trans in self.transformers[feature]:
                    fp = trans.transform(X[:, i], y)
                    if len(fp.shape) == 1:
                        fp = fp.reshape((-1, 1))
                    Xp.append(fp)
        return np.hstack(Xp)

    def fit_transform(self, X, y):
        self.fit(X, y)
        return self.transform(X, y)

def get_clf_scores(y_pred_proba, y, threshold=0.5, scores=None):
    if scores is None:
        scores = {'accuracy': [], 'TNR': [], 'TPR': [], 'AUC': []}
    y_pred = np.zeros(len(y))
    y_pred[y_pred_proba >= threshold] = 1
    scores['accuracy'].append(accuracy_score(y, y_pred))
    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()
    scores['TNR'].append(1. * tn / (tn + fp))
    scores['TPR'].append(1. * tp / (tp + fn))
    fpr, tpr, _ = roc_curve(y, y_pred_proba)
    scores['AUC'].append(auc(fpr, tpr))
    return scores

def run_single_model(guides, model_config):
    logging.info('guides')
    logging.info(str(guides))
    logging.info('model configuration')
    logging.info(str(model_config))
    logging.info('input data')
    data = IconData()
    data.partition_data(test_size=TEST_SIZE, n_splits=N_FOLDS, random_state=RANDOM_SEED)

    logging.info('model training')
    cv_scores = None

    for k in range(N_FOLDS):
        hash_tr, y_tr, X_tr, hash_va, y_va, X_va = data[k]
        trans = FeatureEngineeringPipeline(guides, data.features)
        trans.fit(X_tr, y_tr)
        X_tr, X_va = map(lambda x: trans.transform(x), (X_tr, X_va))

        clf = model_config['model'](**model_config['params'])
        clf.fit(X_tr, y_tr)
        cv_scores = get_clf_scores(clf.predict_proba(X_va)[:, 1], y_va, 0.5, cv_scores)
        logging.info('fold %d, train fold %s, valid fold %s, valid accuracy %f' 
                    % ((k + 1), str(X_tr.shape), str(X_va.shape), cv_scores['accuracy'][-1]))
    valid_score = {k: {'mean': np.mean(v), 'std': np.std(v), 'scores': v} for k, v in cv_scores.items()}

    trans = FeatureEngineeringPipeline(guides, data.features)
    trans.fit(data.X_train, data.y_train)
    X_tr, X_te = map(lambda x: trans.transform(x), (data.X_train, data.X_test))
    clf = model_config['model'](**model_config['params'])
    clf.fit(X_tr, data.y_train)
    y_test_pred_proba = clf.predict_proba(X_te)[:, 1]
    test_score = get_clf_scores(y_test_pred_proba, data.y_test, 0.5)
    test_score = {k: v[0] for k, v in test_score.items()}
    fpr, tpr, _ = roc_curve(data.y_test, y_test_pred_proba)

    roc = {'fpr': fpr, 'tpr': tpr, 'auc': auc(fpr, tpr)}

    return {'valid_score': valid_score, 'test_score': test_score, 'roc': roc}    

if __name__ == '__main__':
    doctest.testmod()